{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rucUc9G56lgy"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW_c1yyl50eQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split, Subset, ConcatDataset\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install optuna\n",
        "import optuna\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTvZ54qa6jf3"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mriwzY16qHC"
      },
      "outputs": [],
      "source": [
        "# --- Dataset variables ---\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 4\n",
        "#ROOT = '/content/drive/My Drive/Colab Notebooks/156 Project/Rice_Image_Dataset'\n",
        "ROOT = '/content/drive/My Drive/156 Project/Rice_Image_Dataset' #this is the root for Farrel\n",
        "\n",
        "\n",
        "FULL_DATASET = ImageFolder(root=ROOT)\n",
        "NUM_CLASSES = len(FULL_DATASET.classes)\n",
        "\n",
        "# --- DEVICE ---\n",
        "DEVICE = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "\n",
        "# --- Train and Validation Set Proportions ---\n",
        "P_TRAIN = 0.6 #0.6\n",
        "P_VAL = 0.2 #0.2\n",
        "P_TUNE = 0.3\n",
        "\n",
        "# --- Training Parameters ---\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 20 #20\n",
        "HIDDEN_SIZE = 32\n",
        "OPTIMIZER = optim.Adam\n",
        "\n",
        "# --- Validation Parameters ---\n",
        "NUM_TRIALS = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue2d1iSn7ixu"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "In this dataset, each of the five classes contributes exactly 15,000 images, so class representation is perfectly balanced across the full distribution. Furthermore, image sizing is consistent where each image is 250x250 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RjaxMUz7cgW"
      },
      "outputs": [],
      "source": [
        "# Generate indices for sampling images from the dataset\n",
        "idx = [np.arange(5)* 15000] * 5 + np.random.choice(15000,(5,5))\n",
        "idx = idx.T\n",
        "\n",
        "# Create a 5x5 grid of subplots\n",
        "fig, ax = plt.subplots(5,5, figsize=(15,20),sharex=True,sharey=True)\n",
        "\n",
        "for r, l in enumerate(idx):\n",
        "    for c, i in enumerate(l):\n",
        "        im,lbl = FULL_DATASET[i]\n",
        "\n",
        "        ax[r,c].imshow(im)\n",
        "        \n",
        "        # Label first column and title third column\n",
        "        if c == 0:\n",
        "            ax[r,c].set_ylabel(f'{FULL_DATASET.classes[lbl]}')\n",
        "        if c == 2:\n",
        "            ax[r,c].set_title(f'{FULL_DATASET.classes[lbl]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7Cngmsy7sIB"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "We compute the mean and standard deviation over the entire dataset and use these values to normalize the inputs. Since mean and standard deviation are low-level, label-agnostic statistics (they do not encode target information directly), estimating them on the full dataset does not introduce any meaningful form of data leakage in this context. Practically, using the full dataset also avoids the extra overhead of recomputing these statistics on the training subset only, while still providing stable, representative normalization parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFFpfikU7vir"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Define preprocessing transforms: resize images and convert to tensor\n",
        "pre_transform = transforms.Compose([\n",
        "    transforms.Resize((250,250)), # Resize all images to 250x250\n",
        "    transforms.ToTensor() # Convert images to PyTorch tensors (0-1 range)\n",
        "])\n",
        "\n",
        "# Load dataset and create DataLoader for batch processing\n",
        "pre_dataset = ImageFolder(root=ROOT, transform=pre_transform)\n",
        "pre_loader = DataLoader(pre_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "# Initialize variables to accumulate mean and std\n",
        "mean = 0.0\n",
        "std = 0.0\n",
        "nb_samples = 0.0\n",
        "\n",
        "# Loop over batches in the DataLoader\n",
        "for d, _ in pre_loader:\n",
        "    batch_samples = d.size(0) # Number of images in current batch\n",
        "    d = d.view(batch_samples, d.size(1), -1) # Flatten HxW into one dimension per channel\n",
        "    mean += d.mean(2).sum(0) # Sum per-channel means\n",
        "    std += d.std(2).sum(0) # Sum per-channel standard deviations\n",
        "    nb_samples += batch_samples # Keep track of total number of samples\n",
        "\n",
        "# Compute final mean and std by dividing by total number of images\n",
        "mean /= nb_samples\n",
        "std /= nb_samples\n",
        "\n",
        "# Print per-channel mean and standard deviation\n",
        "print(f\"mean:{mean}\")\n",
        "print(f'std: {std}')\n",
        "'''\n",
        "\n",
        "mean = [0.1179, 0.1189, 0.1229]\n",
        "std = [0.2851, 0.2875, 0.2989]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHeOfGvd7y3i"
      },
      "source": [
        "## Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNGdZrMF71A6"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((250,250)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((250,250)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean,std=std)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvFNceGO73NA"
      },
      "source": [
        "## Train, Validation, and Test Set Division"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiAPrxXI72Mj"
      },
      "outputs": [],
      "source": [
        "# Compute sizes for train, validation, and test splits\n",
        "total_size = len(FULL_DATASET)\n",
        "train_size = int(P_TRAIN * total_size)\n",
        "val_size = int(P_VAL * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# Randomly split the dataset into train, val, and test subsets\n",
        "train_sub, val_sub, test_sub = random_split(\n",
        "    FULL_DATASET, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42) # Ensure reproducibility\n",
        ")\n",
        "\n",
        "# Get the indices of each subset\n",
        "train_idx = train_sub.indices\n",
        "val_idx   = val_sub.indices\n",
        "test_idx  = test_sub.indices\n",
        "\n",
        "# Create Subset objects with appropriate transforms\n",
        "train_set = Subset(ImageFolder(ROOT, transform=train_transform), train_idx)\n",
        "val_set   = Subset(ImageFolder(ROOT, transform=val_test_transform), val_idx)\n",
        "test_set  = Subset(ImageFolder(ROOT, transform=val_test_transform), test_idx)\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_set,     BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_set,   BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq5FMnS9ZaIf"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "images_flat = [] # List to store flattened images\n",
        "labels_list = [] # List to store corresponding labels\n",
        "\n",
        "print(\"Gathering data for PCA...\")\n",
        "\n",
        "# Loop over validation loader to collect a small subset of images\n",
        "for i, (imgs, lbls) in enumerate(val_loader):\n",
        "    if i > 5: break # Limit to first 6 batches for speed\n",
        "    flat = imgs.view(imgs.size(0), -1).numpy()  # Flatten images to 1D vectors\n",
        "    images_flat.extend(flat)\n",
        "    labels_list.extend(lbls.numpy())\n",
        "\n",
        "# Perform PCA to reduce image dimensions to 2D\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(images_flat)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "df_pca = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
        "df_pca['Rice Type'] = [FULL_DATASET.classes[l] for l in labels_list]\n",
        "\n",
        "# Plot the 2D PCA result with Seaborn\n",
        "sns.scatterplot(x='PC1', y='PC2', hue='Rice Type', data=df_pca, palette='tab10', alpha=0.7)\n",
        "plt.title('PCA of Raw Rice Images (2D Projection)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CetO88jb7-QS"
      },
      "source": [
        "# Creating the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaAE1OaA8ANj"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size):\n",
        "        super(CNN,self).__init__()\n",
        "        hidden_size2 = hidden_size*2  # Double channels in second conv layer\n",
        "\n",
        "         # Feature extractor: 2 conv layers with ReLU and MaxPool\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3,  out_channels=hidden_size, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size2, kernel_size= 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Classifier: global pooling + flatten + linear layer\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(hidden_size2,num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        out = self.classifier(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "537p_sSz8B5s"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY3N2OdM8DYX"
      },
      "outputs": [],
      "source": [
        "# Instantiate the CNN and move it to the device \n",
        "cnn = CNN(NUM_CLASSES,HIDDEN_SIZE)\n",
        "cnn = cnn.to(DEVICE)\n",
        "\n",
        "# Print a summary of the model (layers, output shapes, params)\n",
        "summary(cnn,(3,250,250))\n",
        "\n",
        "# Set up the optimizer for training\n",
        "optimizer = OPTIMIZER(cnn.parameters(),lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIO2rZjn8Fg5"
      },
      "source": [
        "## Evaluate Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOzQ3d9N8JOX"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, device, leave=True):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient computation\n",
        "        for images, labels in tqdm(loader, desc='Evaluation: ', leave=leave):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = CRITERION(outputs, labels) # Compute loss\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Get predicted class and count correct predictions\n",
        "            _, preds = torch.max(outputs,1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            \n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euinH4918Qld"
      },
      "source": [
        "# Training the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZlsAq0D8ONq"
      },
      "outputs": [],
      "source": [
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "val_accuracy_list = []\n",
        "\n",
        "cnn = cnn.to(DEVICE)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    cnn.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{NUM_EPOCHS}]', leave=True)\n",
        "\n",
        "    for images, labels in loop:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad() # Clear previous gradients\n",
        "        outputs = cnn(images) # Forward pass\n",
        "        loss = CRITERION(outputs, labels)  # Compute loss\n",
        "        loss.backward() # Backpropagation\n",
        "        optimizer.step() # Update weights\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        loop.set_postfix(batch_loss=loss.item())  # Display batch loss in tqdm\n",
        "\n",
        "    # Compute average training loss for the epoch\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    # Evaluate on validation set\n",
        "    val_loss, val_acc = evaluate(cnn, val_loader, DEVICE)\n",
        "\n",
        "    # Store metrics for plotting or analysis\n",
        "    train_loss_list.append(train_loss)\n",
        "    val_loss_list.append(val_loss)\n",
        "    val_accuracy_list.append(val_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}]: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lDHcpArKE6C"
      },
      "outputs": [],
      "source": [
        "# Save the trained model's weights to Google Drive\n",
        "drive_path = '/content/drive/My Drive/156 Project/rice_cnn_best_model.pth'\n",
        "torch.save(final_model.state_dict(), drive_path)\n",
        "print(f\"Model saved permanently to {drive_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD3Nc0j8KTld"
      },
      "outputs": [],
      "source": [
        "# Download the saved model file to your local machine\n",
        "from google.colab import files\n",
        "files.download('rice_cnn_best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urf6Amjb5Vul"
      },
      "outputs": [],
      "source": [
        "# Instantiate the same CNN architecture and move to device\n",
        "loaded_model = CNN(num_classes=NUM_CLASSES, hidden_size=32)\n",
        "loaded_model = loaded_model.to(DEVICE)\n",
        "\n",
        "# Path to the saved model weights\n",
        "load_path = '/content/drive/My Drive/156 Project/rice_cnn_best_model.pth'\n",
        "\n",
        "# Load the saved weights into the model\n",
        "loaded_model.load_state_dict(torch.load(load_path, map_location=DEVICE))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLQ8sTdu8HIN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "train_loss_list = [\n",
        "    0.2678, 0.2573, 0.2396, 0.2255, 0.2166,\n",
        "    0.1958, 0.1873, 0.1752, 0.1631, 0.1517,\n",
        "    0.1550, 0.1347, 0.1353, 0.1287, 0.1206,\n",
        "    0.1212, 0.1113, 0.1220, 0.1076, 0.1004\n",
        "]\n",
        "\n",
        "val_loss_list = [\n",
        "    0.4059, 0.1638, 0.1423, 0.1969, 0.0901,\n",
        "    0.0941, 0.1325, 0.1939, 0.0527, 0.0917,\n",
        "    0.0432, 0.0497, 0.0899, 0.0448, 0.0713,\n",
        "    0.1269, 0.0615, 0.0663, 0.0371, 0.0616\n",
        "]\n",
        "\n",
        "val_accuracy_list = [\n",
        "    0.8170, 0.9378, 0.9503, 0.9231, 0.9746,\n",
        "    0.9795, 0.9497, 0.9334, 0.9860, 0.9691,\n",
        "    0.9880, 0.9860, 0.9726, 0.9910, 0.9829,\n",
        "    0.9637, 0.9902, 0.9876, 0.9905, 0.9847\n",
        "]\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "fig, ax = plt.subplots(1, 2, sharex=True, figsize=(12, 5))\n",
        "\n",
        "# Plot Training vs Validation Loss\n",
        "ax[0].plot(range(1, NUM_EPOCHS + 1), train_loss_list, label=\"Training Loss\")\n",
        "ax[0].plot(range(1, NUM_EPOCHS + 1), val_loss_list, label=\"Validation Loss\")\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Training vs Validation Loss')\n",
        "ax[0].legend()\n",
        "ax[0].grid(True)\n",
        "\n",
        "# Plot Validation Accuracy\n",
        "ax[1].plot(range(1, NUM_EPOCHS + 1), val_accuracy_list, label=\"Validation Accuracy\", color='green')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].set_title('Validation Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgjr-PzZ8V95"
      },
      "source": [
        "## Visuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPq2sLQE8bhC"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2, sharex=True)\n",
        "\n",
        "# Plot training and validation loss\n",
        "ax[0].plot(range(NUM_EPOCHS), train_loss_list,   label = \"Training Loss\")\n",
        "ax[0].plot(range(NUM_EPOCHS), val_loss_list,     label = 'Validation Loss')\n",
        "\n",
        "# Plot validation accuracy\n",
        "ax[1].plot(range(NUM_EPOCHS), val_accuracy_list, label = 'Validation Accuracy')\n",
        "\n",
        "# Set axis labels and titles\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[0].set_title('Loss')\n",
        "ax[1].set_title('Accuracy')\n",
        "\n",
        "# Add legends\n",
        "ax[0].legend()\n",
        "ax[1].legend()\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "fig.tight_layout()\n",
        "\n",
        "# Print rounded metrics\n",
        "print(np.round(train_loss_list,3))\n",
        "print(np.round(val_loss_list,3))\n",
        "print(np.round(val_accuracy_list,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rieTX8yE8cql"
      },
      "source": [
        "# Validation and Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaTX99Q88f7S"
      },
      "outputs": [],
      "source": [
        "# Select a random subset of the training set for tuning \n",
        "indices = np.random.choice(len(train_set), int(len(train_set)*P_TUNE), replace=False) \n",
        "\n",
        "# Create a Subset and corresponding DataLoader for tuning\n",
        "tune_train_set = Subset(train_set, indices)\n",
        "tune_train_loader = DataLoader(tune_train_set, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlrYUJsv8qu2"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Suggest hyperparameters for this trial\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "    hidden_size = trial.suggest_categorical('hidden_size', [16,32,64])\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "\n",
        "    # Create and move CNN to device\n",
        "    cnn = CNN(NUM_CLASSES, hidden_size)\n",
        "    cnn = cnn.to(DEVICE)\n",
        "    optimizer = OPTIMIZER(cnn.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "\n",
        "    # Train for a few epochs on the tuning subset\n",
        "    for epoch in range(3):\n",
        "        cnn.train()\n",
        "        for images, labels in tune_train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn(images)\n",
        "            loss = CRITERION(outputs,labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on validation set and report to Optuna\n",
        "        val_loss, val_acc = evaluate(cnn, val_loader, DEVICE)\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune(): # Early stopping for unpromising trials\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    # Final evaluation on validation set\n",
        "    val_loss, val_acc = evaluate(cnn, val_loader, DEVICE, leave=False)\n",
        "    return val_loss # Objective: minimize validation loss\n",
        "\n",
        "# Set up Optuna study with pruning\n",
        "pruner = optuna.pruners.MedianPruner()\n",
        "study = optuna.create_study(direction='minimize',pruner=pruner)\n",
        "study.optimize(objective, n_trials=NUM_TRIALS, show_progress_bar=True)\n",
        "\n",
        "# Print and save best hyperparameters\n",
        "print(\"Best trial:\", study.best_trial.params)\n",
        "best = study.best_trial.params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTADaOtU8sWl"
      },
      "source": [
        "# Testing the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndI-pISc8xDW"
      },
      "source": [
        "## Retraining the Entire Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFC3QyhE8vqM"
      },
      "outputs": [],
      "source": [
        "# Best hyperparameters from Optuna\n",
        "best = {'lr': 0.001, 'hidden_size': 32, 'weight_decay': 1e-5}\n",
        "best_hidden = best['hidden_size']\n",
        "best_lr = best['lr']\n",
        "best_wd = best['weight_decay']\n",
        "\n",
        "# Initialize final model with best hyperparameters\n",
        "final_model = CNN(NUM_CLASSES, best_hidden)\n",
        "final_model = final_model.to(DEVICE)\n",
        "\n",
        "# Optimizer for final training\n",
        "final_optimizer = OPTIMIZER(final_model.parameters(), lr = best_lr, weight_decay = best_wd)\n",
        "\n",
        "# Combine training and validation sets for final training\n",
        "train_val_set = ConcatDataset([train_set,val_set])\n",
        "train_val_loader = DataLoader(train_val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "# Train final model on combined dataset\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    final_model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(train_val_loader, desc=f'Final Training Epoch [{epoch+1}/{NUM_EPOCHS}]'):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        final_optimizer.zero_grad()\n",
        "        outputs = final_model(images)\n",
        "        loss = CRITERION(outputs, labels)\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(train_val_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}, Combined Train Loss: {epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY8YypT7pKKE"
      },
      "source": [
        "Save the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZeYmtH8pMqM"
      },
      "outputs": [],
      "source": [
        "# Save the trained final model's weights\n",
        "model_save_path = 'rice_cnn_best_model.pth'\n",
        "torch.save(final_model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd4Gn3FO83UD"
      },
      "source": [
        "## Testing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8CwbHvT8560"
      },
      "outputs": [],
      "source": [
        "# Evaluate the final trained model on the test set\n",
        "test_loss, test_acc = evaluate(final_model, test_loader, DEVICE)\n",
        "print(f\"Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whZC8j4DoSIm"
      },
      "source": [
        "##Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTVdI0PuoaIy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Collect predictions and true labels from the test set\n",
        "for images, labels in test_loader:\n",
        "  images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "  outputs = final_model(images)\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  all_preds.extend(preds.cpu().numpy())\n",
        "  all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=FULL_DATASET.classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA9qM1zkBfhZ"
      },
      "outputs": [],
      "source": [
        "# Extract the best hidden size from the hyperparameter tuning results\n",
        "best = {'lr': 0.001, 'hidden_size': 32, 'weight_decay': 1e-5}\n",
        "best_hidden = best['hidden_size']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0-RbOiOUvQL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torch\n",
        "import numpy as np\n",
        "y_pred = []\n",
        "y_true = []\n",
        "final_model.eval() # Set model to evaluation mode\n",
        "print(\"Generating predictions...\")\n",
        "\n",
        "with torch.no_grad(): # Disable gradient computation\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = final_model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1) # Get predicted class indices\n",
        "\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "print(\"Predictions generated successfully.\")\n",
        "\n",
        "# Print detailed classification metrics\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_true, y_pred, target_names=FULL_DATASET.classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXFU8TsTWoWB"
      },
      "source": [
        "The \"Basmati vs. Jasmine\" Confusion is Quantified\n",
        "Basmati Recall (0.80): This is our lowest score. It means 20% of real Basmati rice is being missed. Based on our Confusion Matrix, we know they are being mislabeled as Jasmine.\n",
        "\n",
        "Jasmine Precision (0.77): This is also low. It means that when our model claims \"This is Jasmine,\" it is only right 77% of the time. Why? Because it's \"hallucinating\" Jasmine when it actually sees Basmati.\n",
        "\n",
        "\n",
        "While the model achieved an overall accuracy of 92%, performance was non-uniform across classes. Specifically, the model struggled to distinguish Basmati from Jasmine, resulting in a lower recall for Basmati (0.80) and lower precision for Jasmine (0.77). This suggests the model relies heavily on features (likely grain length or color) that are shared between these two specific rice varieties.\n",
        "\n",
        "Ipsala is the \"Control Group\"\n",
        "Ipsala (Precision 1.00, Recall 0.96): The model is basically perfect at identifying Ipsala.\n",
        "\n",
        "This proves our model architecture works well. The failure on Basmati isn't because our CNN is broken; it's because the data (the images of those specific grains) is harder to distinguish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82XyrlPYdgw0"
      },
      "outputs": [],
      "source": [
        "# Visualize Data Augmentation Effects\n",
        "def visualize_augmentation(dataset, idx=0):\n",
        "    raw_img, label = dataset[idx] # Original image and its label\n",
        "\n",
        "    fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
        "\n",
        "    # Placeholder for original image\n",
        "    axes[0].text(0.5, 0.5, \"Original\\n(See Raw)\", ha='center')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Generate and display augmented images\n",
        "    for i in range(1, 6):\n",
        "        img_aug, _ = train_set[idx] # Apply training transforms\n",
        "\n",
        "        # Undo normalization for visualization\n",
        "        img_display = img_aug.clone()\n",
        "        mean = [0.1179, 0.1189, 0.1229]\n",
        "        std = [0.2851, 0.2875, 0.2989]\n",
        "        for c in range(3):\n",
        "            img_display[c] = img_display[c] * std[c] + mean[c]\n",
        "\n",
        "        # Convert to HxWxC and clip values\n",
        "        img_display = img_display.permute(1, 2, 0).numpy()\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "        axes[i].imshow(img_display)\n",
        "        axes[i].set_title(f\"Augmentation {i}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Effect of Data Augmentation on {FULL_DATASET.classes[label]}\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize augmentations on the 101st image\n",
        "visualize_augmentation(FULL_DATASET, idx=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nPHcObGhJbU"
      },
      "outputs": [],
      "source": [
        "# Transform for test images: resize, tensor, normalize\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((250,250)),\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize(mean=[0.1179, 0.1189, 0.1229], std=[0.2851, 0.2875, 0.2989])\n",
        "])\n",
        "\n",
        "# Class indices\n",
        "basmati_idx = FULL_DATASET.class_to_idx['Basmati']\n",
        "jasmine_idx = FULL_DATASET.class_to_idx['Jasmine']\n",
        "\n",
        "print(\"Scanning for Basmati -> Jasmine errors... (This might take a moment)\")\n",
        "found_images = []\n",
        "final_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(FULL_DATASET)):\n",
        "        if len(found_images) >= 5: break\n",
        "\n",
        "\n",
        "        path, label = FULL_DATASET.samples[i]\n",
        "\n",
        "        # Only check Basmati images\n",
        "        if label == basmati_idx:\n",
        "            img_tensor, _ = FULL_DATASET[i]\n",
        "\n",
        "            # Load raw PIL image and apply test transforms\n",
        "            img_raw = FULL_DATASET.loader(path) # Load raw PIL\n",
        "            img_input = test_transform(img_raw).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            # Predict class\n",
        "            output = final_model(img_input)\n",
        "            _, pred = torch.max(output, 1)\n",
        "\n",
        "            # If misclassified as Jasmine, save the image\n",
        "            if pred.item() == jasmine_idx:\n",
        "                found_images.append(img_input.cpu().squeeze(0))\n",
        "\n",
        "print(f\"Found {len(found_images)} misclassified examples.\")\n",
        "\n",
        "# Visualize misclassified images\n",
        "if len(found_images) > 0:\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    for i, img_tensor in enumerate(found_images):\n",
        "        ax = plt.subplot(1, 5, i+1)\n",
        "\n",
        "        # Un-normalize\n",
        "        img_display = img_tensor.clone()\n",
        "        mean = [0.1179, 0.1189, 0.1229]\n",
        "        std = [0.2851, 0.2875, 0.2989]\n",
        "        for c in range(3):\n",
        "            img_display[c] = img_display[c] * std[c] + mean[c]\n",
        "\n",
        "        img_display = img_display.permute(1, 2, 0).numpy()\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "        ax.imshow(img_display)\n",
        "        ax.set_title(\"True: Basmati\\nPred: Jasmine\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.suptitle(\"Error Analysis: Basmati grains misclassified as Jasmine\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No errors found in the scanned samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd_H8k6Dlini"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "feature_list = []\n",
        "label_list = []\n",
        "\n",
        "final_model.eval()\n",
        "print(\"Extracting CNN features...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(DEVICE)\n",
        "\n",
        "        # Pass through the convolutional layers\n",
        "        x = final_model.features(images)\n",
        "        x = final_model.classifier[0](x) # AdaptiveAvgPool\n",
        "        x = final_model.classifier[1](x) # Flatten\n",
        "\n",
        "        feature_list.extend(x.cpu().numpy())\n",
        "        label_list.extend(labels.numpy())\n",
        "\n",
        "features = np.array(feature_list)\n",
        "labels = np.array(label_list)\n",
        "\n",
        "# Perform K-Means clustering on extracted features\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "clusters = kmeans.fit_predict(features)\n",
        "\n",
        "# Evaluate clustering quality with Adjusted Rand Index\n",
        "ari = adjusted_rand_score(labels, clusters)\n",
        "print(f\"K-Means Adjusted Rand Index (clustering quality): {ari:.4f}\")\n",
        "\n",
        "# Reduce features to 2D for visualization\n",
        "pca_features = PCA(n_components=2).fit_transform(features)\n",
        "\n",
        "# Plot 2D PCA scatter colored by cluster assignment\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(pca_features[:, 0], pca_features[:, 1], c=clusters, cmap='tab10', alpha=0.6)\n",
        "plt.title(f'K-Means Clustering of CNN Features (Bishop Sec 9.1)\\nARI Score: {ari:.3f}')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(scatter, label='Cluster ID')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
