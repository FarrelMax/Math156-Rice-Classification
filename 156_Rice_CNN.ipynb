{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rucUc9G56lgy"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW_c1yyl50eQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split, Subset, ConcatDataset\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "!pip install optuna\n",
        "import optuna\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTvZ54qa6jf3"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mriwzY16qHC"
      },
      "outputs": [],
      "source": [
        "# --- Dataset variables ---\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 4\n",
        "#ROOT = '/content/drive/My Drive/Colab Notebooks/156 Project/Rice_Image_Dataset'\n",
        "ROOT = '/content/drive/My Drive/156 Project/Rice_Image_Dataset' #this is the root for Farrel\n",
        "\n",
        "\n",
        "FULL_DATASET = ImageFolder(root=ROOT)\n",
        "NUM_CLASSES = len(FULL_DATASET.classes)\n",
        "\n",
        "# --- DEVICE ---\n",
        "DEVICE = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "\n",
        "# --- Train and Validation Set Proportions ---\n",
        "P_TRAIN = 0.6 #0.6\n",
        "P_VAL = 0.2 #0.2\n",
        "P_TUNE = 0.3\n",
        "\n",
        "# --- Training Parameters ---\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 20 #20\n",
        "HIDDEN_SIZE = 32\n",
        "OPTIMIZER = optim.Adam\n",
        "\n",
        "# --- Validation Parameters ---\n",
        "NUM_TRIALS = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue2d1iSn7ixu"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "In this dataset, each of the five classes contributes exactly 15,000 images, so class representation is perfectly balanced across the full distribution. Furthermore, image sizing is consistent where each image is 250x250 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RjaxMUz7cgW"
      },
      "outputs": [],
      "source": [
        "idx = [np.arange(5)* 15000] * 5 + np.random.choice(15000,(5,5))\n",
        "idx = idx.T\n",
        "\n",
        "fig, ax = plt.subplots(5,5, figsize=(15,20),sharex=True,sharey=True)\n",
        "\n",
        "for r, l in enumerate(idx):\n",
        "    for c, i in enumerate(l):\n",
        "        im,lbl = FULL_DATASET[i]\n",
        "\n",
        "        ax[r,c].imshow(im)\n",
        "        if c == 0:\n",
        "            ax[r,c].set_ylabel(f'{FULL_DATASET.classes[lbl]}')\n",
        "        if c == 2:\n",
        "            ax[r,c].set_title(f'{FULL_DATASET.classes[lbl]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7Cngmsy7sIB"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "We compute the mean and standard deviation over the entire dataset and use these values to normalize the inputs. Since mean and standard deviation are low-level, label-agnostic statistics (they do not encode target information directly), estimating them on the full dataset does not introduce any meaningful form of data leakage in this context. Practically, using the full dataset also avoids the extra overhead of recomputing these statistics on the training subset only, while still providing stable, representative normalization parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFFpfikU7vir"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "pre_transform = transforms.Compose([\n",
        "    transforms.Resize((250,250)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "pre_dataset = ImageFolder(root=ROOT, transform=pre_transform)\n",
        "pre_loader = DataLoader(pre_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "mean = 0.0\n",
        "std = 0.0\n",
        "nb_samples = 0.0\n",
        "\n",
        "for d, _ in pre_loader:\n",
        "    batch_samples = d.size(0)\n",
        "    d = d.view(batch_samples, d.size(1), -1)\n",
        "    mean += d.mean(2).sum(0)\n",
        "    std += d.std(2).sum(0)\n",
        "    nb_samples += batch_samples\n",
        "\n",
        "mean /= nb_samples\n",
        "std /= nb_samples\n",
        "\n",
        "print(f\"mean:{mean}\")\n",
        "print(f'std: {std}')\n",
        "'''\n",
        "\n",
        "mean = [0.1179, 0.1189, 0.1229]\n",
        "std = [0.2851, 0.2875, 0.2989]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHeOfGvd7y3i"
      },
      "source": [
        "## Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNGdZrMF71A6"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((250,250)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((250,250)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean,std=std)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvFNceGO73NA"
      },
      "source": [
        "## Train, Validation, and Test Set Division"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiAPrxXI72Mj"
      },
      "outputs": [],
      "source": [
        "total_size = len(FULL_DATASET)\n",
        "train_size = int(P_TRAIN * total_size)\n",
        "val_size = int(P_VAL * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_sub, val_sub, test_sub = random_split(\n",
        "    FULL_DATASET, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "train_idx = train_sub.indices\n",
        "val_idx   = val_sub.indices\n",
        "test_idx  = test_sub.indices\n",
        "\n",
        "train_set = Subset(ImageFolder(ROOT, transform=train_transform), train_idx)\n",
        "val_set   = Subset(ImageFolder(ROOT, transform=val_test_transform), val_idx)\n",
        "test_set  = Subset(ImageFolder(ROOT, transform=val_test_transform), test_idx)\n",
        "\n",
        "train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)\n",
        "val_loader = DataLoader(val_set,     BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_set,   BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "images_flat = []\n",
        "labels_list = []\n",
        "\n",
        "print(\"Gathering data for PCA...\")\n",
        "for i, (imgs, lbls) in enumerate(val_loader):\n",
        "    if i > 5: break\n",
        "    flat = imgs.view(imgs.size(0), -1).numpy()\n",
        "    images_flat.extend(flat)\n",
        "    labels_list.extend(lbls.numpy())\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(images_flat)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "df_pca = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
        "df_pca['Rice Type'] = [FULL_DATASET.classes[l] for l in labels_list]\n",
        "\n",
        "sns.scatterplot(x='PC1', y='PC2', hue='Rice Type', data=df_pca, palette='tab10', alpha=0.7)\n",
        "plt.title('PCA of Raw Rice Images (2D Projection)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Iq5FMnS9ZaIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CetO88jb7-QS"
      },
      "source": [
        "# Creating the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaAE1OaA8ANj"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size):\n",
        "        super(CNN,self).__init__()\n",
        "        hidden_size2 = hidden_size*2\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3,  out_channels=hidden_size, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size2, kernel_size= 5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(hidden_size2,num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        out = self.classifier(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "537p_sSz8B5s"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY3N2OdM8DYX"
      },
      "outputs": [],
      "source": [
        "cnn = CNN(NUM_CLASSES,HIDDEN_SIZE)\n",
        "cnn = cnn.to(DEVICE)\n",
        "summary(cnn,(3,250,250))\n",
        "optimizer = OPTIMIZER(cnn.parameters(),lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIO2rZjn8Fg5"
      },
      "source": [
        "## Evaluate Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOzQ3d9N8JOX"
      },
      "outputs": [],
      "source": [
        "# TODO: Add more metrics, maybe specifically just for test set, so create a new function?\n",
        "def evaluate(model, loader, device, leave=True):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc='Evaluation: ', leave=leave):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = CRITERION(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs,1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euinH4918Qld"
      },
      "source": [
        "# Training the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZlsAq0D8ONq"
      },
      "outputs": [],
      "source": [
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "val_accuracy_list = []\n",
        "\n",
        "cnn = cnn.to(DEVICE)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    cnn.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{NUM_EPOCHS}]', leave=True)\n",
        "\n",
        "    for images, labels in loop:\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cnn(images)\n",
        "        loss = CRITERION(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        loop.set_postfix(batch_loss=loss.item())\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    val_loss, val_acc = evaluate(cnn, val_loader, DEVICE)\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "    val_loss_list.append(val_loss)\n",
        "    val_accuracy_list.append(val_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}]: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}' )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = '/content/drive/My Drive/156 Project/rice_cnn_best_model.pth'\n",
        "torch.save(final_model.state_dict(), drive_path)\n",
        "print(f\"Model saved permanently to {drive_path}\")"
      ],
      "metadata": {
        "id": "7lDHcpArKE6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('rice_cnn_best_model.pth')"
      ],
      "metadata": {
        "id": "HD3Nc0j8KTld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loaded_model = CNN(num_classes=NUM_CLASSES, hidden_size=32)\n",
        "loaded_model = loaded_model.to(DEVICE)\n",
        "\n",
        "\n",
        "load_path = '/content/drive/My Drive/156 Project/rice_cnn_best_model.pth'\n",
        "\n",
        "\n",
        "loaded_model.load_state_dict(torch.load(load_path, map_location=DEVICE))\n",
        "\n",
        "loaded_model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "urf6Amjb5Vul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "train_loss_list = [\n",
        "    0.2678, 0.2573, 0.2396, 0.2255, 0.2166,\n",
        "    0.1958, 0.1873, 0.1752, 0.1631, 0.1517,\n",
        "    0.1550, 0.1347, 0.1353, 0.1287, 0.1206,\n",
        "    0.1212, 0.1113, 0.1220, 0.1076, 0.1004\n",
        "]\n",
        "\n",
        "val_loss_list = [\n",
        "    0.4059, 0.1638, 0.1423, 0.1969, 0.0901,\n",
        "    0.0941, 0.1325, 0.1939, 0.0527, 0.0917,\n",
        "    0.0432, 0.0497, 0.0899, 0.0448, 0.0713,\n",
        "    0.1269, 0.0615, 0.0663, 0.0371, 0.0616\n",
        "]\n",
        "\n",
        "val_accuracy_list = [\n",
        "    0.8170, 0.9378, 0.9503, 0.9231, 0.9746,\n",
        "    0.9795, 0.9497, 0.9334, 0.9860, 0.9691,\n",
        "    0.9880, 0.9860, 0.9726, 0.9910, 0.9829,\n",
        "    0.9637, 0.9902, 0.9876, 0.9905, 0.9847\n",
        "]\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 20\n",
        "fig, ax = plt.subplots(1, 2, sharex=True, figsize=(12, 5))\n",
        "\n",
        "# Plot Losses\n",
        "ax[0].plot(range(1, NUM_EPOCHS + 1), train_loss_list, label=\"Training Loss\")\n",
        "ax[0].plot(range(1, NUM_EPOCHS + 1), val_loss_list, label=\"Validation Loss\")\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Training vs Validation Loss')\n",
        "ax[0].legend()\n",
        "ax[0].grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "ax[1].plot(range(1, NUM_EPOCHS + 1), val_accuracy_list, label=\"Validation Accuracy\", color='green')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].set_title('Validation Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JLQ8sTdu8HIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgjr-PzZ8V95"
      },
      "source": [
        "## Visuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPq2sLQE8bhC"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2, sharex=True)\n",
        "ax[0].plot(range(NUM_EPOCHS), train_loss_list,   label = \"Training Loss\")\n",
        "ax[0].plot(range(NUM_EPOCHS), val_loss_list,     label = 'Validation Loss')\n",
        "ax[1].plot(range(NUM_EPOCHS), val_accuracy_list, label = 'Validation Accuracy')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[0].set_title('Loss')\n",
        "ax[1].set_title('Accuracy')\n",
        "ax[0].legend()\n",
        "ax[1].legend()\n",
        "fig.tight_layout()\n",
        "\n",
        "print(np.round(train_loss_list,3))\n",
        "print(np.round(val_loss_list,3))\n",
        "print(np.round(val_accuracy_list,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rieTX8yE8cql"
      },
      "source": [
        "# Validation and Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaTX99Q88f7S"
      },
      "outputs": [],
      "source": [
        "indices = np.random.choice(len(train_set), int(len(train_set)*P_TUNE), replace=False) # tuning on subset of training dataset\n",
        "tune_train_set = Subset(train_set, indices)\n",
        "tune_train_loader = DataLoader(tune_train_set, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlrYUJsv8qu2"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "    hidden_size = trial.suggest_categorical('hidden_size', [16,32,64])\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "\n",
        "    cnn = CNN(NUM_CLASSES, hidden_size)\n",
        "    cnn = cnn.to(DEVICE)\n",
        "    optimizer = OPTIMIZER(cnn.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "\n",
        "    for epoch in range(3):\n",
        "        cnn.train()\n",
        "        for images, labels in tune_train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn(images)\n",
        "            loss = CRITERION(outputs,labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        val_loss, val_acc = evaluate(cnn, val_loader, DEVICE)\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    val_loss, val_acc = evaluate(cnn, val_loader, DEVICE, leave=False)\n",
        "    return val_loss\n",
        "\n",
        "pruner = optuna.pruners.MedianPruner()\n",
        "study = optuna.create_study(direction='minimize',pruner=pruner)\n",
        "study.optimize(objective, n_trials=NUM_TRIALS, show_progress_bar=True)\n",
        "print(\"Best trial:\", study.best_trial.params)\n",
        "best = study.best_trial.params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTADaOtU8sWl"
      },
      "source": [
        "# Testing the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndI-pISc8xDW"
      },
      "source": [
        "## Retraining the Entire Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFC3QyhE8vqM"
      },
      "outputs": [],
      "source": [
        "best = {'lr': 0.001, 'hidden_size': 32, 'weight_decay': 1e-5}\n",
        "best_hidden = best['hidden_size']\n",
        "best_lr = best['lr']\n",
        "best_wd = best['weight_decay']\n",
        "\n",
        "final_model = CNN(NUM_CLASSES, best_hidden)\n",
        "final_model = final_model.to(DEVICE)\n",
        "\n",
        "final_optimizer = OPTIMIZER(final_model.parameters(), lr = best_lr, weight_decay = best_wd)\n",
        "\n",
        "train_val_set = ConcatDataset([train_set,val_set])\n",
        "train_val_loader = DataLoader(train_val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    final_model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(train_val_loader, desc=f'Final Training Epoch [{epoch+1}/{NUM_EPOCHS}]'):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        final_optimizer.zero_grad()\n",
        "        outputs = final_model(images)\n",
        "        loss = CRITERION(outputs, labels)\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(train_val_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}, Combined Train Loss: {epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model\n"
      ],
      "metadata": {
        "id": "PY8YypT7pKKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'rice_cnn_best_model.pth'\n",
        "torch.save(final_model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "EZeYmtH8pMqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd4Gn3FO83UD"
      },
      "source": [
        "## Testing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8CwbHvT8560"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_loss, test_acc = evaluate(final_model, test_loader, DEVICE)\n",
        "print(f\"Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whZC8j4DoSIm"
      },
      "source": [
        "##Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTVdI0PuoaIy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "for images, labels in test_loader:\n",
        "  images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "  outputs = final_model(images)\n",
        "  _, preds = torch.max(outputs, 1)\n",
        "  all_preds.extend(preds.cpu().numpy())\n",
        "  all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=FULL_DATASET.classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best = {'lr': 0.001, 'hidden_size': 32, 'weight_decay': 1e-5}\n",
        "best_hidden = best['hidden_size']"
      ],
      "metadata": {
        "id": "oA9qM1zkBfhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torch\n",
        "import numpy as np\n",
        "y_pred = []\n",
        "y_true = []\n",
        "final_model.eval()\n",
        "print(\"Generating predictions...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = final_model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "print(\"Predictions generated successfully.\")\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_true, y_pred, target_names=FULL_DATASET.classes))"
      ],
      "metadata": {
        "id": "L0-RbOiOUvQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Basmati vs. Jasmine\" Confusion is Quantified\n",
        "Basmati Recall (0.80): This is our lowest score. It means 20% of real Basmati rice is being missed. Based on our Confusion Matrix, we know they are being mislabeled as Jasmine.\n",
        "\n",
        "Jasmine Precision (0.77): This is also low. It means that when our model claims \"This is Jasmine,\" it is only right 77% of the time. Why? Because it's \"hallucinating\" Jasmine when it actually sees Basmati.\n",
        "\n",
        "\n",
        "While the model achieved an overall accuracy of 92%, performance was non-uniform across classes. Specifically, the model struggled to distinguish Basmati from Jasmine, resulting in a lower recall for Basmati (0.80) and lower precision for Jasmine (0.77). This suggests the model relies heavily on features (likely grain length or color) that are shared between these two specific rice varieties.\n",
        "\n",
        "Ipsala is the \"Control Group\"\n",
        "Ipsala (Precision 1.00, Recall 0.96): The model is basically perfect at identifying Ipsala.\n",
        "\n",
        "This proves our model architecture works well. The failure on Basmati isn't because our CNN is broken; it's because the data (the images of those specific grains) is harder to distinguish."
      ],
      "metadata": {
        "id": "PXFU8TsTWoWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Data Augmentation Effects\n",
        "def visualize_augmentation(dataset, idx=0):\n",
        "    raw_img, label = dataset[idx] #\n",
        "\n",
        "    fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
        "\n",
        "    axes[0].text(0.5, 0.5, \"Original\\n(See Raw)\", ha='center')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    for i in range(1, 6):\n",
        "        img_aug, _ = train_set[idx]\n",
        "\n",
        "        img_display = img_aug.clone()\n",
        "        mean = [0.1179, 0.1189, 0.1229]\n",
        "        std = [0.2851, 0.2875, 0.2989]\n",
        "        for c in range(3):\n",
        "            img_display[c] = img_display[c] * std[c] + mean[c]\n",
        "\n",
        "        img_display = img_display.permute(1, 2, 0).numpy()\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "        axes[i].imshow(img_display)\n",
        "        axes[i].set_title(f\"Augmentation {i}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Effect of Data Augmentation on {FULL_DATASET.classes[label]}\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_augmentation(FULL_DATASET, idx=100)"
      ],
      "metadata": {
        "id": "82XyrlPYdgw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((250,250)),\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize(mean=[0.1179, 0.1189, 0.1229], std=[0.2851, 0.2875, 0.2989])\n",
        "])\n",
        "basmati_idx = FULL_DATASET.class_to_idx['Basmati']\n",
        "jasmine_idx = FULL_DATASET.class_to_idx['Jasmine']\n",
        "\n",
        "print(\"Scanning for Basmati -> Jasmine errors... (This might take a moment)\")\n",
        "found_images = []\n",
        "final_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(FULL_DATASET)):\n",
        "        if len(found_images) >= 5: break\n",
        "\n",
        "\n",
        "        path, label = FULL_DATASET.samples[i]\n",
        "\n",
        "        if label == basmati_idx:\n",
        "            img_tensor, _ = FULL_DATASET[i]\n",
        "\n",
        "            img_raw = FULL_DATASET.loader(path) # Load raw PIL\n",
        "            img_input = test_transform(img_raw).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            output = final_model(img_input)\n",
        "            _, pred = torch.max(output, 1)\n",
        "\n",
        "            if pred.item() == jasmine_idx:\n",
        "                found_images.append(img_input.cpu().squeeze(0))\n",
        "\n",
        "print(f\"Found {len(found_images)} misclassified examples.\")\n",
        "\n",
        "if len(found_images) > 0:\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    for i, img_tensor in enumerate(found_images):\n",
        "        ax = plt.subplot(1, 5, i+1)\n",
        "\n",
        "        # Un-normalize\n",
        "        img_display = img_tensor.clone()\n",
        "        mean = [0.1179, 0.1189, 0.1229]\n",
        "        std = [0.2851, 0.2875, 0.2989]\n",
        "        for c in range(3):\n",
        "            img_display[c] = img_display[c] * std[c] + mean[c]\n",
        "\n",
        "        img_display = img_display.permute(1, 2, 0).numpy()\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "        ax.imshow(img_display)\n",
        "        ax.set_title(\"True: Basmati\\nPred: Jasmine\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.suptitle(\"Error Analysis: Basmati grains misclassified as Jasmine\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No errors found in the scanned samples\")"
      ],
      "metadata": {
        "id": "1nPHcObGhJbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "feature_list = []\n",
        "label_list = []\n",
        "\n",
        "final_model.eval()\n",
        "print(\"Extracting CNN features...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(DEVICE)\n",
        "\n",
        "        # Pass through the convolutional layers\n",
        "        x = final_model.features(images)\n",
        "        x = final_model.classifier[0](x) # AdaptiveAvgPool\n",
        "        x = final_model.classifier[1](x) # Flatten\n",
        "\n",
        "        feature_list.extend(x.cpu().numpy())\n",
        "        label_list.extend(labels.numpy())\n",
        "\n",
        "features = np.array(feature_list)\n",
        "labels = np.array(label_list)\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "clusters = kmeans.fit_predict(features)\n",
        "\n",
        "ari = adjusted_rand_score(labels, clusters)\n",
        "print(f\"K-Means Adjusted Rand Index (clustering quality): {ari:.4f}\")\n",
        "\n",
        "pca_features = PCA(n_components=2).fit_transform(features)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(pca_features[:, 0], pca_features[:, 1], c=clusters, cmap='tab10', alpha=0.6)\n",
        "plt.title(f'K-Means Clustering of CNN Features (Bishop Sec 9.1)\\nARI Score: {ari:.3f}')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(scatter, label='Cluster ID')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dd_H8k6Dlini"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}